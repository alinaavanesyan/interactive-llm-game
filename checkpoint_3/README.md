# Метаданные

> **Тетрадка с кодом:** rag_game.ipynb
> **Аугментированные данные:** script_augmented.txt
> **Валидационный датасет:** valid_dataset.xlsx* (или по [ссылке](https://docs.google.com/spreadsheets/d/18yd8-6-t5_yrtRpj_PKtTvY-N1LyHLjOp9kOguKkUSc/edit?usp=sharing))

# Отчёт по Чекпоинту 3

В рамках нашего проекта мы создаём визуальную новеллу с адаптивным сюжетом, где ответы NPC должны строго соответствовать их оригинальным характерам. Для реализации этой задумки мы планируем использовать RAG-систему, которая будет извлекать фразы персонажей из транскриптов анимационного сериала “BoJack Horseman”, обеспечивая доступ к фактам, эмоциональным состояниям, особенностям речи и динамике отношений между героями.

Для нашего пайплайна критически важно:
- чтобы LLM выбирала правильные факты;
- чтобы ответы были в стиле конкретного персонажа;
- чтобы система могла поддерживать когерентность социального контекста: отношения, эмоции, мотивацию.

## 1. Baseline-решение

**Архитектура RAG-системы**

Для retrieval использовалась Chroma + эмбеддинги all-MiniLM-L6-v2.

Для генерации ответов — модель llama-3.1-8b-instant через Groq API.

**Аугментация данных:**

Для увеличения объёма и разнообразия реплики были использованы сгенерированные диалоги с помощью Gemini 3 Pro Preview. Аугментация позволила:

- увеличить вероятность нахождения релевантных фрагментов при поиске;
- повысить покрытие редких или сложных ситуаций;
- улучшить качество генерации ответов при редких вопросах.


## 2. Валидация


### Датасет:

Для валидации RAG-системы мы подготовили собственный датасет следующего вида:

| question                                           | right answer   | right chunk                                               | rag answer                                                      | rag chunk                                                 |
|---------------------------------------------------|----------------|-----------------------------------------------------------|-----------------------------------------------------------------|----------------------------------------------------------|
| How many seasons did the show "Horsin' Around" air? | Nine seasons.  | the family comedy struck a chord with America and went on to air for nine seasons. | The show "Horsin' Around" went on to air for nine seasons. | the family comedy struck a chord with America and went on to air for nine seasons. |

где: 
- question — вопрос по тексту
- right_answer — верный ответ
- right_chunk — фрагмент текста, позволяющий дать ответ
- rag_answer — ответ RAG-системы
- rag_chunks — найденные системой релевантные фрагменты

Объём датасета: 200+ вопросов.

Вопросы и соответствующие им ответы были сгенерированы с помощью модели Gemini 3 Pro Preview (обращались к модели через открытый интерфейс ai.studio от Google). Специфика этой модели заключаются в большом размере context window (до 1М токенов), что позволяет ей эффективно работать с длинными последовательностями. На вход модели мы подавали транскрипты из мультфильма. 

Валидационные вопросы мы разделили на категории, чтобы проверить способность RAG-системы извлекать определенные типы контекста, критически важные для отыгрыша роли:
1. Character Identity & Core Facts – базовые, неизменные факты о персонаже
2. Relationship Dynamics – контекст взаимодействия персонажей друг с другом (дружба, вражда, романтика).
3. Internal Context & Emotional State – информация об эмоциональном состоянии героев, об их психологических проблемах и страхах
4. Plot & Motivation – цели, действия персонажей, сюжетные решения

Вопросы подбирались таким образом, чтобы:
- они имели чёткий, однозначный ответ — например, да/нет, имя персонажа, профессия, название локации;
- допускали точную формулировку ответа (например, конкретная цитата персонажа или описательный факт);
- включали более сложные композиционные запросы, состоящие из двух или более частей (например: “Who is Pinky and where does he work?”).

Такой подбор вопросов позволяет проверить разные аспекты системы: от базового извлечения фактов до способности модели сопоставлять несколько элементов информации и обобщать.

### Метрики

Т.к. наша система основана на RAG-подходе, мы оцениваем её по двум независимым направлениям:

**1. Retrieval (поиск чанков)**

- **Top-1 Accuracy** — доля вопросов, для которых top-1 найденный чанк совпадает с эталонным;
- **Top-K Accuracy (K=5)** — доля вопросов, для которых правильный чанк встречается среди top-5 найденных;
- **Semantic Similarity** — максимальная косинусная схожесть между эталонным чанкoм и найденными.

**2. Generation (качество ответа модели)**

- **F1-score** — учитывает пересечение токенов в ответе и эталоне;
- **Semantic Similarity** — косинусная близость между ответом модели и эталонным;
- **ROUGE-L** — метрика, учитывающая последовательность слов и порядок, подходит для частично переформулированных ответов;

**Главная метрика:** `retrieval_top1`, так как корректный выбор контекста критически важен для того, чтобы LLM воспроизводила личность персонажа.

## 3. Результаты baseline-решения

| Метрика                        | Значение |
|--------------------------------|----------|
| Retrieval Top-1                 | 0.21    |
| Retrieval Top-5                 | 0.36    |
| Retrieval Semantic Similarity   | 0.55    |
| Generation F1                   | 0.12    |
| Generation Semantic Similarity  | 0.31    |
| Generation ROUGE-L              | 0.15    |


### Выводы по метрикам:

#### Анализ Retrieval:

   - **Top-1 Accuracy = 0.21**, **Top-5 Accuracy = 0.36**, **Semantic Similarity = 0.55**  
   - Это указывает на то, что система часто выбирает не самый релевантный фрагмент или не полностью совпадающий с эталонным.  
   - Поскольку в RAG генерация зависит от контекста, низкая точность retrieval напрямую ограничивает качество ответа LLM.  

#### Анализ генерации:
   - Важно понимать, что строгие метрики, такие как F1 или Exact Match, плохо отражают качество для нашей задачи визуальной новеллы, где ответы могут быть корректными, но переформулированными или расширенными:
     
     > **Пример:**  
     > - right_answer: `Nine seasons.`  
     > - rag_answer: `The show "Horsin' Around" went on to air for nine seasons.`  
     > - F1 ≈ 0.12, ROUGE-L ≈ 0.15, Semantic Similarity ≈ 0.31  
     → ответ правильный по смыслу, но метрики дают низкое значение, потому что не учитывают переформулировку и дополнительные детали.  
   
   - Семантические метрики (ROUGE-L, Embedding Similarity) дают более реалистичную оценку, однако слонны занижать оценки. Возможно, отчасти поэтому мы получили не очень высокие метрики по части генерации.

   - Использование Gemini 3 Pro для генерации дополнительных реплик увеличило разнообразие контекста, однако текущая семантическая близость и Top-K метрики показывают, что этого недостаточно для качественного покрытия всех вопросов.  
   - Похоже, что для редких или сложных ситуаций необходимо ещё больше аугментированных данных и/или более продвинутые методы embedding + retrieval.


**Главная метрика для дальнейшей работы** — `semantic similarity` между сгенерированным ответом и эталоном.
   - остальные метрики генерации склонны недооценивать качество генерации, особенно когда ответы правильные по смыслу.  
   - Cтрогий Top-1 retrieval не является самой важной метрикой, так как не все ответы должны строго совпадать с одной репликой.   
   - Top-K retrieval метрики остаются полезными для анализа, но не должны определять успех генерации.  

 **Рекомендации для дальнейших шагов**  
   - Улучшить retrieval: более мощные эмбеддинги, расширение базы данных, учитывать роль персонажа и сцену.  
   - Сосредоточиться на генерации: модели должны сохранять смысл, стиль и контекст даже при переформулировке.  
   - Использовать семантические метрики (ROUGE-L, embedding similarity) как ключевые для оценки качества, Top-K retrieval — как вспомогательные.  
   - Продолжить аугментацию данных, особенно для редких сцен и составных вопросов.  


## 4. Текущие проблемы и идеи по улучшению пайплайна

1. **Проблемы с retrieval:**
   - Низкие показатели Top-1 и Top-5 (0.21 и 0.36) показывают, что система часто не выбирает наиболее релевантный контекст.  
   - Существующие эмбеддинги (all-MiniLM-L6-v2) могут быть недостаточно точными для сложных или редких сцен.  

   **Идеи для улучшения:**
   - Использовать более мощные эмбеддинговые модели (например, MiniLM v2, SBERT-large) или комбинированные dense + sparse retrieval подходы (т.е. добавить к поиску по эмбеддингам фильтрацию по ключевым словам, например, по именам персонажей). 
   - Добавить контекстные признаки (роль персонажа, сцена, эмоции) при индексировании и поиске.  
   - Расширить базу аугментированных чанков (путём генерации материала, а также парсинга транскриптов других серий мультсериала.  

2. **Проблемы с генерацией и оценкой качества генерации:**
   - Низкие F1 (0.12), ROUGE-L (0.15) и Semantic Similarity (0.31) отражают, что модель часто переформулирует или дополняет ответы, что не учитывается токенными метриками.  
   - Для визуальной новеллы важен стиль и характер персонажа, а не строгое совпадение токенов.  
   - Текущие метрики недооценивают ответы, которые верны по смыслу, но переформулированы. Это особенно критично для новеллы, где допустимы вариации формулировки и расширения ответов.
     
   **Идеи для улучшения:**
   - Сделать основной упор на семантическую оценку генерации (Embedding Similarity, ROUGE-L), а F1 использовать как вспомогательную метрику.  

Текущее baseline-решение показывает, что система способна извлекать и генерировать информацию, но пока слишком нестабильна для полноценного использования.
