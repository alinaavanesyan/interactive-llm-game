# Отчёт по Чекпоинту 3

В рамках нашего проекта мы создаём визуальную новеллу с адаптивным сюжетом, где ответы NPC должны строго соответствовать их оригинальным характерам. Для реализации этой задумки мы планируем использовать RAG-систему, которая будет извлекать фразы персонажей из транскриптов анимационного сериала “BoJack Horseman”, обеспечивая доступ к фактам, эмоциональным состояниям, особенностям речи и динамике отношений между героями.

Для нашего пайплайна критически важно:
- чтобы LLM выбирала правильные факты;
- чтобы ответы были в стиле конкретного персонажа;
- чтобы система могла поддерживать когерентность социального контекста: отношения, эмоции, мотивацию.

## Baseline-решение

**Архитектура RAG-системы**

Для retrieval использовалась Chroma + эмбеддинги all-MiniLM-L6-v2.

Для генерации ответов — модель llama-3.1-8b-instant через Groq API.

**Аугментация данных:**

Для увеличения объёма и разнообразия реплики были использованы сгенерированные диалоги с помощью Gemini 3 Pro Preview. Аугментация позволила:

- увеличить вероятность нахождения релевантных фрагментов при поиске;
- повысить покрытие редких или сложных ситуаций;
- улучшить качество генерации ответов при редких вопросах.


## Валидация


### Датасет:

Для валидации RAG-системы мы подготовили собственный датасет следующего вида:

| question                                           | right answer   | right chunk                                               | rag answer                                                      | rag chunk                                                 |
|---------------------------------------------------|----------------|-----------------------------------------------------------|-----------------------------------------------------------------|----------------------------------------------------------|
| How many seasons did the show "Horsin' Around" air? | Nine seasons.  | the family comedy struck a chord with America and went on to air for nine seasons. | The show "Horsin' Around" went on to air for nine seasons. | the family comedy struck a chord with America and went on to air for nine seasons. |

где: 
- question — вопрос по тексту
- right_answer — верный ответ
- right_chunk — фрагмент текста, позволяющий дать ответ
- rag_answer — ответ RAG-системы
- rag_chunks — найденные системой релевантные фрагменты

Объём датасета: 200+ вопросов (доступен в *valid_dataset.xlsx*, а также по [ссылке](https://docs.google.com/spreadsheets/d/18yd8-6-t5_yrtRpj_PKtTvY-N1LyHLjOp9kOguKkUSc/edit?usp=sharing)).

Вопросы и соответствующие им ответы были сгенерированы с помощью модели Gemini 3 Pro Preview (обращались к модели через открытый интерфейс ai.studio от Google). Специфика этой модели заключаются в большом размере context window (до 1М токенов), что позволяет ей эффективно работать с длинными последовательностями. На вход модели мы подавали транскрипты из мультфильма. 

Валидационные вопросы мы разделили на категории, чтобы проверить способность RAG-системы извлекать определенные типы контекста, критически важные для отыгрыша роли:
1. Character Identity & Core Facts – базовые, неизменные факты о персонаже
2. Relationship Dynamics – контекст взаимодействия персонажей друг с другом (дружба, вражда, романтика).
3. Internal Context & Emotional State – информация об эмоциональном состоянии героев, об их психологических проблемах и страхах
4. Plot & Motivation – цели, действия персонажей, сюжетные решения

Вопросы подбирались таким образом, чтобы:
- они имели чёткий, однозначный ответ — например, да/нет, имя персонажа, профессия, название локации;
- допускали точную формулировку ответа (например, конкретная цитата персонажа или описательный факт);
- включали более сложные композиционные запросы, состоящие из двух или более частей (например: “Who is Pinky and where does he work?”).

Такой подбор вопросов позволяет проверить разные аспекты системы: от базового извлечения фактов до способности модели сопоставлять несколько элементов информации и обобщать.

### Метрики

Т.к. наша система основана на RAG-подходе, мы оцениваем её по двум независимым направлениям:

**1. Retrieval (поиск чанков)**

- **Top-1 Accuracy** — доля вопросов, для которых top-1 найденный чанк совпадает с эталонным;
- **Top-K Accuracy (K=5)** — доля вопросов, для которых правильный чанк встречается среди top-5 найденных;
- **Semantic Similarity** — максимальная косинусная схожесть между эталонным чанкoм и найденными.

**2. Generation (качество ответа модели)**

- **F1-score** — учитывает пересечение токенов в ответе и эталоне;
- **Semantic Similarity** — косинусная близость между ответом модели и эталонным;
- **ROUGE-L** — метрика, учитывающая последовательность слов и порядок, подходит для частично переформулированных ответов;
- **Exact Match (EM)** — бинарная метрика для строго совпадающих ответов (используется как вспомогательная).

**Главная метрика:** `retrieval_top1`, так как корректный выбор контекста критически важен для того, чтобы LLM воспроизводила личность персонажа.

### Результаты baseline-решения

| Метрика                        | Значение |
|--------------------------------|----------|
| Retrieval Top-1                 | 0.21    |
| Retrieval Top-5                 | 0.36    |
| Retrieval Semantic Similarity   | 0.55    |
| Generation F1                   | 0.12    |
| Generation Semantic Similarity  | 0.31    |
| Generation ROUGE-L              | 0.15    |


#### Выводы по метрикам:

 **Качество генерации низкое по строгим метрикам**  
   - **F1 = 0.12**, **ROUGE-L = 0.15** — модель в большинстве случаев переформулирует ответы или пропускает часть информации.  
   - **Semantic Similarity = 0.31** — ответ модели часто семантически далёк от эталона.  

   **Вывод:** генерация пока больше «импровизация» модели на основе контекста, чем точное воспроизведение фактов. Это ожидаемо, учитывая низкую точность извлечения чанков.

   - Использование Gemini 3 Pro для генерации дополнительных реплик увеличило разнообразие контекста, однако текущая семантическая близость и Top-K метрики показывают, что этого недостаточно для качественного покрытия всех вопросов.  
   - Похоже, что для редких или сложных ситуаций необходимо ещё больше аугментированных данных и/или более продвинутые методы embedding + retrieval.

   - **Главная метрика для дальнейшей работы** — `retrieval_top1`.  
   - Без корректного выбора контекста LLM не сможет отвечать достоверно, даже если генеративная модель качественная.  
   - На следующих этапах основное усилие должно быть направлено на повышение точности извлечения (например, через более мощные Sentence Transformers, Dense Passage Retrieval или расширение базы аугментированных чанков).

6. **Рекомендации для следующих шагов**  
   - Улучшить retrieval: использовать более мощные модели эмбеддингов, увеличить размер индекса, добавить контекстные признаки (роль персонажа, сцена).  
   - Интегрировать **семантические метрики генерации** (ROUGE-L, Semantic Similarity) в качестве основной меры качества вместо F1/EM, так как ответы часто переформулируются.  
   - Продолжить аугментацию данных, особенно для редких сценариев и составных вопросов.  
   - Ввести метрики **style-consistency** и **faithfulness**, чтобы дополнительно оценивать, насколько ответы соответствуют характеру персонажей и канону.

**Итог:** текущее baseline-решение показывает, что система способна извлекать и генерировать информацию, но пока слишком нестабильна для полноценного использования. Главная проблема — низкое качество retrieval, что ограничивает точность генерации. Дальнейшие улучшения должны сосредоточиться на усилении извлечения релевантного контекста и увеличении семантического покрытия базы данных.
Если хочешь, я могу прямо добавить этот анализ в полн
